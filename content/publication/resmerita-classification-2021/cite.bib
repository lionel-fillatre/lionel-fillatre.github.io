@inproceedings{resmerita_classification_2021,
 abstract = {Deep neural networks need to be compressed due to their high memory requirements and computational complexity. Previous papers have proposed numerous methods to quantize the weights with a single bit or more. However, the loss of accuracy involved in the compression is scarcely studied from a theoretical point of view. Motivated by the rate-distortion theory, we propose a new distortion measure which assesses the gap between the Bayes risk of a classifier before and after the compression. Since this distortion is not easily tractable, we derive a theoretical approximation when the last fully connected layer of a deep neural network is compressed under the assumption that the layer inputs follow a multivariate normal distribution. Numerical results show that the approximation performs well on both synthetic and real data. We also show that heuristic quantizers proposed in the literature may not be optimal.},
 author = {Resmerita, Diana and Farias, Rodrigo Cabral and Fillatre, Lionel},
 booktitle = {2021 29th European Signal Processing Conference (EUSIPCO)},
 doi = {10.23919/EUSIPCO54536.2021.9616015},
 file = {Snapshot:/Users/fillatre/Zotero/storage/YNKU2FGV/9616015.html:text/html},
 keywords = {Analytical models, Deep learning, Europe, Gaussian distribution, Memory management, Quantization (signal), Rate-distortion},
 month = {August},
 note = {ISSN: 2076-1465},
 pages = {1446--1450},
 title = {Classification Error Approximation of a Compressed Linear Softmax Layer},
 url = {https://ieeexplore.ieee.org/document/9616015},
 urldate = {2025-04-25},
 year = {2021}
}
